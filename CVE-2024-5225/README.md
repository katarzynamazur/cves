# :mag_right: Background
LiteLLM is an open-source locally run proxy server that provides an OpenAI-compatible API. It interfaces with a large number of providers that do the inference. To handle the inference, a popular open-source inference engine is Ollama.

# :bug: Vulnerability Detail
An SQL Injection vulnerability exists in the [https://github.com/BerriAI/litellm](BerriAI/LiteLLM) repository (in versions < 1.40.0), specifically within the `/global/spend/logs` endpoint. The vulnerability arises due to improper neutralization of special elements used in an SQL command. The affected code constructs an SQL query by concatenating an unvalidated `api_key` parameter directly into the query, making it susceptible to SQL Injection if the `api_key` contains malicious data. This issue affects the latest version of the repository. Successful exploitation of this vulnerability could lead to unauthorized access, data manipulation, exposure of confidential information, and denial of service (DoS).

# :bulb: Workaround
There is no known workaround for this vulnerability. It is essential to update to the latest version of LiteLLM to address the issue.

# Conclusion
The SQL Injection vulnerability in LiteLLM can have severe consequences, including unauthorized access, data manipulation, exposure of confidential information, and denial of service (DoS). It is crucial to update to the latest version of LiteLLM (1.40.1 or later) to mitigate the risk.

## :whale: How to test for the vulnerability using Docker?

```
docker compose --file docker-compose-cve-2024-5225.yaml up
```

